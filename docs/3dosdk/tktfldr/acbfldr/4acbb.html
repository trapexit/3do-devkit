<TITLE>Writing efficient C for the ARM</TITLE>
<H1><LINK NAME="XREF40955">Writing efficient C for the ARM</LINK></H1><HR>
<H2>About this recipe</H2>
The ARM C compiler can generate very good machine code for if you present 
it with the right sort of input. From this note, you will learn:<P>
<UL>
<LI>what the C compiler compiles well and why;<P>
<LI>how to help the C compiler to generate excellent machine code.
</UL>
Some of the rules of thumb presented are quite general; some are quite 
specific to the ARM or the ARM C compiler. It should be quite clear from 
context which rules are portable.<P>
The first subsection below is concerned with how to design collections of 
C functions to maximise low-level efficiency. The following subsection is 
concerned with the efficiency of larger and more complicated functions.<P>
<H2>Function design considerations</H2>
Unlike on many earlier CISC processor architectures, function call 
overhead on the ARM is small and often in proportion to the work done by 
the called function. Several feaures contribute to this:<P>
<UL>
<LI>the minimal ARM call-return sequence is BL... MOV pc, lr, which is 
extremely economical;<P>
<LI>STM and LDM reduce the cost of entry to and exit from functions which 
must create a stack frame and/or save registers;<P>
<LI>the ARM Procedure Call Standard has been carefully designed to allow 
two very important types of function call to be optimised so that the 
entry and exit overheads are minimal.
</UL>
Good general advice is to keep functions small, because function calling 
overheads are low. In the remainder of this subsection you will learn 
precisely when function call overhead is very low. In following 
subsections you will learn how small functions help the ARM C compiler; 
you will also learn how to assist the C compiler when functions cannot be 
kept small.<P>
<H3>Leaf functions</H3>
In 'typical' programs, about half of all function calls made are to leaf 
functions (a leaf function is one which makes no calls from within its 
body).<P>
Often, a leaf function is rather simple. On the ARM, if it is simple 
enough to compile using just 5 registers (a1-a4 and ip), it will carry no 
function entry or exit overhead. A surprising proportion of useful leaf 
functions can be compiled within this constraint.<P>
Once registers have to be saved, it is efficient to save them using STM. 
In fact the more you can save at one go, the better. In a leaf function, 
all and only the registers which need to be saved will be saved by a 
single STMFD sp!,{regs,lr} on entry and a matching LDMFD sp!,{regs,pc} on 
exit.<P>
In general, the cost of pushing some registers on entry and popping them 
on exit is very small compared to the cost of the useful work done by a 
leaf function which is complicated enough to need more than 5 registers.<P>
Overall, you should expect a leaf function to carry virtually no function 
entry and exit overhead; and at worst, a small overhead, most likely in 
proportion to the useful work done by it.<P>
<H3>Veneer functions (Simple fail continued functions)</H3>
Historically, abstraction veneers have been relatively expensive. The kind 
of veneer function which merely changes the types of its arguments, or 
which calls a low-level implementation with an extra argument (say), has 
often cost much more in entry and exit overhead than it was worth in 
useful work.<P>
On the ARM, if a function ends with a call to another function, that call 
can be converted to a <B>tail continuation</B>. In functions which need to 
save no registers, the effect can be dramatic. Consider, for example:<P>
<CODE>
<PRE>
extern void *__sys_alloc(unsigned type, unsigned n_words);
#define  NOTGCable   0x80000000
#define  NOTMovable  0x40000000

void *malloc(unsigned n_bytes)
{   
    return __sys_alloc(NOTGCable+NOTMovable, n_bytes/4);
}
</CODE>
</PRE>
Here, <B>armcc</B> generates:<P>
<CODE>
<PRE>
malloc
    MOV     a2,a1,LSR #2
    MOV     a1,#&amp;c0000000
    B       |__sys_alloc|
</CODE>
</PRE>
There is no function entry or exit overhead-just useful work massaging 
arguments-and the function return has disappeared entirely - return is 
direct from __sys_alloc to malloc's caller. In this case, the basic 
call-return cost for the function pair has been reduced from:<P>
<CODE>
<PRE>
BL + BL + MOV pc,lr + MOV pc,lr
</CODE>
</PRE>
to:<P>
<CODE>
<PRE>
BL + B  +             MOV pc,lr
</CODE>
</PRE>
a saving of 25%.<P>
More complicated functions in which the only function calls are 
immediately before a return, collapse equally well. An artificial example 
is:<P>
<CODE>
<PRE>
extern int f1(int), int f2(int, int);
</CODE>
</PRE>

<CODE>
<PRE>
int f(int a, int b)
{   
    if (b == 0)
        return a;
    else if (b &lt; 0)
        return f2(a, -b);
    else
        return f2(b, a);                              /* argument order 
swapped */
}
</CODE>
</PRE>
<B>armcc</B> generates the following, wonderfully efficient code:<P>
<CODE>
<PRE>
f    CMP     a2,#0
    MOVEQS  pc,lr
    RSBLT   a2,a2,#0
    BLT     f2
    MOV     a3,a1
    MOV     a1,a2
    MOV     a2,a3
    B       f2
</CODE>
</PRE>
<H3>Fast paths and slow paths - A useful transformation</H3>
Inevitably, not all functions can be leaves or small abstraction 
functions. And, inevitably, non-leaf functions must carry the cost of 
establishing a call frame on entry and removing it on exit, perhaps also 
the cost of saving and restoring some registers. How does this hurt 
performance? Consider the following example:<P>
<CODE>
<PRE>
int f(Buffer *b)
{    if (b-&gt;n &gt; 0)
     {   /* The usual path through the function... */
         /*     95% of all calls.*/
         /* Simple calculation involving b-&gt;buf, b-&gt;n, etc.*/
         return ...;
     }
     /* Exceptional path through the function... */
     /*     5% of all calls.  */
     /* Complicated calculation involving calls
     /*     to other functions.*/
     return ...;
}
</CODE>
</PRE>
In this case, the entry and register-save overhead caused by the 
infrequent heavyweight path through the function applies to the much more 
frequent lightweight path through it. To fix this, turn the heavyweight 
path into a tail call. Yes, introducing another layer of function call 
yields much more efficient code!<P>
<CODE>
<PRE>
int f2(Buffer *b)
{     /* Exceptional path through the function... */
      /*     5% of all calls.  */
      /* Complicated calculation involving calls */
      /*     to other functions.*/
      return ...;
}
</CODE>
</PRE>
<CODE>
<PRE>
int f(Buffer *b)
{    if (b-&gt;n &gt; 0)
         {     /* The usual path through the function... */
               /*     95% of all calls.*/
               /* Simple calculation involving b-&gt;buf, b-&gt;n, etc.*/
               return ...;]
        }
    return f2(b);
}
</CODE>
</PRE>
If you are lucky, f() will now compile using only a1-a4 and ip and so 
incur no entry overhead whatsoever. 95% of the time, the overhead on the 
original f() will be reduced to zero.<P>
This is quite a general source transformation technique and you should 
look for opportunities to use it and analogous transformations. It works 
for any processor to some extent; it works particulary well for the ARM 
because of the careful optimisation of tail continuation in lightweight 
functions.<P>
Repeated application of this technique to the chain of six or so functions 
called for every character processed by the preprocessing phase of the ARM 
C compiler, improved the performance of the preprocessor (running on the 
ARM) by about 30%.<P>
<H3>Function arguments and argument passing</H3>
The final aspect of function design which influences low-level efficiency 
is argument passing.<P>
Under the ARM Procedure Call Standard, up to four argument words can be 
passed to a function in registers. Functions of up to four integral (not 
floating point) arguments are particularly efficient and incur very little 
overhead beyond that required to compute the argument expressions 
themselves (there may be a little register juggling in the called 
function, depending on its complexity).<P>
If more arguments are needed, then the 5th, 6th, etc., words will be 
passed on the stack. This incurs the cost of an STR in the calling 
function and an LDR in the called function for each argument word beyond 
four.<P>
How can argument passing overhead be minimised?<P>
<UL>
<LI>Try to ensure that small functions take four or fewer arguments. These 
will compile particualrly well.<P>
<LI>If a function needs many arguments, try to ensure that it does a 
significant amount of work on every call, so that the cost of passing 
arguments is amortised.<P>
<LI>Factor out read-mostly global control state and make this static. If 
it has to be passed as an argument (e.g. to support multiple clients) then 
wrap it up in a struct and pass a pointer to it. The characteristics of 
such control state are:
</UL>
<DL>
<DD>
<UL>
<LI>it's logically global to the compilation unit or program<P>
<LI>it's read-mostly, often read-only except in response to user input, 
and for almost all functions cannot be changed by them or any function 
called from them;<P>
<LI>references to it are ubiquitous, but in any function, references are 
relatively rare (frequent references should be replaced by references to a 
local, non-static copy).
</UL><BR>
</DL>
<UL>
<LI>Don't confuse such control state with compuational arguments, the 
values of which differ on every call.
</UL>
<UL>
<LI>Collect related data into structs. Decide whether to pass pointers or 
struct values based on the use of each struct in the called function:
</UL>
<DL>
<DD>
<UL>
<LI>If few fields are read or written then passing a pointer is best.<P>
<LI>The cost of passing a struct via the stack is typically a share in an 
LDM-STM pair for each word of the struct. This can be better than passing 
a pointer if (i) on average, each field is used at least once and (ii) the 
register pressure in the function is high enough to force a pointer to be 
repeatedly re-loaded. 
</UL><BR>
</DL>
<UL>
<LI>As a rule of thumb, you can't lose much efficiency if you pass 
pointers to structs rather than struct values. To gain efficiency by 
passing struct values rather than pointers usually requires careful study 
of a function's machine code.
</UL>
<H2>Register allocation and how to help it</H2>
It is well known that register allocation is critical to the efficiency of 
code compiled for RISC processors. It is particularly critical for the 
ARM, which has only 16 registers rather than the 'traditional' 32.<P>
The ARM C compiler has a highly efficient register allocator which 
operates on complete functions and which tries to allocate the most 
frequently used variables to registers (taking loop nesting into account). 
It produces very good results unless the demand for registers seriously 
outstrips supply. And it has one shortcoming, namely that it allocates 
whole variables to registers, not separate live ranges.<P>
As code generation proceeds for a function, new variables are created for 
expression temporaries. These are never reused in later expressions and 
cannot be spilled to memory. Usually, this causes no problems. However, a 
particularly pathological expression could, in principle, occupy most of 
the allocatable registers, forcing almost all program variables to be 
spilled to memory. Because the number of registers required to evaluate an 
expression is a logarithmic function of the number terms in it, it takes 
an expression of more than 32 terms to threaten the use of any variable 
register.<P>
As a rule of thumb, avoid very large expressions (more than 30 terms).<P>
The more serious problem is with long scope program variables. Our 
allocator either allocates a variable to a chosen register everywhere the 
variable is live, or it leaves the variable in memory. To help visualise 
the problem - and to see how to help the allocator - consider the 
following two program schemata:<P>
<CODE>
<PRE>
int f()                            int f()
{   int i, j, ...;                 {   int j, ...;
                                     { int i;
    for (i = 0;  i &lt; lim;  ++i)        for (i = 0;  i &lt; lim;  ++i)
    {                                  {
        ...                               ...
    }                                  }
                                     }
                                     { int i;
    for (i = 0;  i &lt; lim;  ++i)        for (i = 0;  i &lt; lim;  ++i)
    {  /* register pressure in this    {
       loop forces 'i' to memory */
    }                                  }
                                     }
                                     { int i;
    for (i = 0;  i &lt; lim;  ++i)        for (i = 0;  i &lt; lim;  ++i)
    {                                  {
        ...                                ...
    }                                  }
                                     }
}                                  }
</CODE>
</PRE>
In the left hand case, because the scope of 'i' is the whole function, if 
'i' cannot be allocated to a register everywhere then all three loops will 
suffer their loop index being in memory. On the other hand, in the right 
hand case there are three separate variables called 'i', each of which 
will be allocated separately by the register allocator.<P>
As a rule of thumb, keep variable declarations local, especially in large 
functions. Use additional block structure as illustrated here (right hand 
example), if necessary.<P>
On the other hand, if this transformation is carried to excess, there may 
be bad results. When a local variable is spilled to memory, there is a 
stack adjustment on each entry to and exit from its containing scope. The 
ARM C compiler does this to minimise the space used by local variables. 
Suppose, for example, that in the right hand case above, each block 
declared a 1KB buffer as well as 'i'. Then adjusting the stack at every 
scope leads to stack usage of just over 1KB whereas adjusting it only at 
function entry leads to usage of more than 3KB.<P>
In principle, the compiler could be more intelligent about adjusting the 
stack locally for large variables and only at function entry for small 
variables. For the moment, the programmer must be aware of these issues.<P>
So, a modified rule of thumb is to cluster variable declarations into 
reasonable sub-scopes within large functions and to avoid doing so within 
the most deeply nested loops. This will most likely help the allocator 
without introducing unwanted costs associated with local stack 
adjustment.<P>
<H2>Static and extern variables - minimising access costs</H2>
A variable in a register costs nothing to access: it is just there, 
waiting to be used. A local (auto) variable is addressed via the sp 
register, which is always available for the purpose.<P>
A static variable, on the other hand, can only be accessed after the 
static base for the compilation unit has been loaded. So, the first such 
use in a function always costs 2 LDRs or an LDR and an STR. However, if 
there are many uses of static variables within a function then there is a 
good chance that the static base will become a global common subexpression 
(CSE) and that, overall, access to static variables will be no more 
expensive than to auto variables on the stack.<P>
Extern variables are fundamentally more expensive: each has its own base 
pointer. Thus each access to an extern is likely to cost 2 LDRs or an LDR 
and an STR. It is much less likely that a pointer to an extern will become 
a global CSE - and almost certain that there cannot be several such CSEs - 
so if a function accesses lots of extern variables, it is bound to incur 
significant access costs.<P>
A further cost occurs when a function is called: the compiler has to 
assume - in the absence of inter-procedural data flow analysis - that 
<B>any</B> non- const static or extern variable <B>could</B> be 
side-effected by the call. This severly limits the scope across which the 
value of a static or extern variable can be held in a register.<P>
Sometimes a programmer can do better than a compiler could do, even a 
compiler that did interprocedural data flow analysis. An example in C is 
given by the standard streams: stdin, stdout and stderr. These are not 
pointers to const objects (the underlying FILE structs are modified by I/O 
operations), nor are they necessarily const pointers (they may be 
assignable in some implementations). Nonetheless, a function can almost 
always safely slave a reference to a stream in a local FILE * variable.<P>
It is a common programming paradigm to mimic the standard streams in 
applications. Consider, for example, the shape of a typical non-leaf 
printing function:<P>
<CODE>
<PRE>
extern FILE *out;                  extern FILE *out;
/* the output stream */            /* the output stream */

void print_it(Thing *t)            void print_it(Thing *t)
{                                  {   FILE *f = out;
    fprintf(out, ...);                 fprintf(f, ...);
    print_1(t-&gt;first);                 print_1(t-&gt;first);
    fprintf(out, ...);                 fprintf(f, ...);
    print_2(t-&gt;second);                print_2(t-&gt;second);
    fprintf(out, ...);                 fprintf(f, ...);
    ...                                ...
}                                  }
</CODE>
</PRE>
In the left hand case, out has be be re-computed or re-loaded after each 
call to print_... (and after each fprintf...). In the right hand case, 'f' 
can be held in a register throughout the function (and probably will 
be).<P>
Uniform application of this transformation to the disassembly module of 
the ARM C compiler saved more than 5% of its code space.<P>
In general, it is difficult and potentially dangerous to assert that no 
function you call (or any functions they in turn call) can affect the 
value of any static or extern variables of which you currently have local 
copies. However, the rewards can be considerable so it is usually 
worthwhile to work out at the program design stage which global variables 
are slavable locally and which are not. Trying to retrofit this 
improvement to exisiting code is usually hazardous, except in very simple 
cases like the above.<P>
<H2>The switch() statement</H2>
The switch() statement can be used to transfer control to one of several 
destinations - conceptually an indexed transfer of control - or to 
generate a value related to the controlling expression (in effect 
computing an in-line function of the controlling expression).<P>
In the first role, switch() is hard to improve upon: the ARM C compiler 
does a good job of deciding when to compile jump tables and when to 
compile trees of if-then-elses. It is rare for a programmer to be able to 
improve upon this by writing if-then-else trees explicitly in the 
source.<P>
In the second role, however, use of switch() is often mistaken. You can 
probably do better by being more aware of what is being computed and 
how.<P>
In the example below, which is abstracted from an early version of the 
disassembly module of the ARM C Compiler, you will learn:<P>
<UL>
<LI>the cost of implementing an in-line function using switch();
<LI>how to implement the same function more economically.
</UL>
The function below used for illustrative purposes maps a 4-bit field of an 
ARM instruction to a 2-character condition code mnemonic. The real case 
was more complicated, decoding two 4-bit fields to a 3-char mnemonic, but 
for illustration the simple example serves just as well. The real case was 
also embedded in a larger function, but this is irrelevant to the 
discussion.<P>
<CODE>
<PRE>
char *cond_of_instr(unsigned instr)
{   
    char *s;`
        switch (instr &amp; 0xf0000000)
        {
case 0x00000000:  s = "EQ";  break;
case 0x10000000:  s = "NE";  break;
     ...          ...        ...
case 0xF0000000:  s = "NV";  break;
        }
        return s;
}
</CODE>
</PRE>
The compiler handles this code fragment well, generating 276 bytes of code 
and string literals. But we could do better. If performance were not 
critical (as it never is in disassembly) then we could look up the code in 
a table of codes, in something like:<P>
<CODE>
<PRE>
char *cond_of_instr(unsigned instr)
{
    static struct {char name[3];  unsigned code;}
        conds[] = {
            "EQ", 0x00000000,
            "NE", 0x10000000,
             ....
             "NV", 0xf0000000,
        };
    int j;
    for (j = 0;  j &lt; sizeof(conds)/sizeof(conds[0]);  ++j)
        if ((instr &amp; 0xf0000000) == conds[j].code)
            return conds[j].name;
    return "";
</CODE>
</PRE>
<CODE>
<PRE>
}
</CODE>
</PRE>
This fragment compiles to 68 bytes of code and 128 bytes of table data. 
Already this is a 30% improvement on the switch() case, but this schema 
has other advantages: it copes well with a random code to string mapping 
and if the mapping is not random admits further optimisation. For example, 
if the code is stored in a byte (char) instead of an unsigned and the 
comparison is with (instr &gt;&gt; 28) rather than (instr &amp; 
0xF0000000) then only 60 bytes of code and 64 bytes of data are generated 
for a total of 124 bytes.<P>
Another advantage we have heard of for table lookup is that is is possible 
to share the same table between a disassembler and an assembler - the 
assembler looks up the mnemonic to obtain the code value, rather than the 
code value to obtain the mnemonic. Where performance is not critical, the 
symmetric property of lookup tables can sometimes be exploited to yield 
significant space savings.<P>
Finally, by exploiting the denseness of the indexing and the uniformity of 
the returned value it is possible to do better again, both in size and 
performance, by direct indexing:<P>
<CODE>
<PRE>
char *cond_of_instr(unsigned instr)
{
    return "\
EQ\0\0NE\0\0CC\0\0CS\0\0MI\0\0PL\0\0VS\0\0VC\0\0\
HI\0\0LS\0\0GE\0\0LT\0\0GT\0\0LE\0\0AL\0\0NV" + (instr &gt;&gt; 28)*4;
}
</CODE>
</PRE>
This expression of the problem causes a miserly 16 bytes of code and 64 
bytes of string literal to be generated and is probably close to what an 
experienced assembly language programmer would naturally write if asked to 
code this function. It is the solution finally adopted in the ARM C 
compiler's disassembler module.<P>
The uniform application of this transformation to the disassembler module 
of the ARM C compiler saved between 5% and 10% of its code space.<P>
The moral of this tale is to think before using switch() to compute an 
in-line function, especially if code size is an important consideration. 
Switch() compiles to high performance code but often table lookup will be 
smaller; where the function's domain is dense, or piecewise dense, direct 
indexing into a table will often be both faster and smaller.<P>
<H2>Related topics</H2>

<UL>
<LI><A HREF="./2acbk.html#XREF12134">ARM assembly programming performance 
issues</A>.
<LI><A HREF="./3acba.html#XREF36909">Register usage under the ARM 
procedure call standard</A>.
<LI><A HREF="./3acbb.html#XREF32701">Passing and returning structs</A>.
</UL>
